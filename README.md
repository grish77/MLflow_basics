# MLOps
MLOps, an acronym for "Machine Learning Operations," is a technique for managing and deploying machine learning (ML) models that integrates ML and AI techniques with software engineering and DevOps (Development Operations) principles. MLOps is based on DevOps, a current method for creating, deploying, and managing enterprise applications more effectively. Software development teams (the Devs) and IT operations teams (the Ops) came together to form DevOps in order to break down data silos and improve collaboration. All these aims of DevOps are similar to MLOps, but MLOps adds Data Scientists and ML Engineers in the team to organize datasets and perform analyses on them using AI algorithms. These individuals also run the datasets through the models using automated, structured procedures.
The entire lifespan of ML models, including their development, training, deployment, and monitoring, is the focus of MLOps. To effectively achieve machine learning model lifecycle management, MLOps fosters the communication and collaboration between operations professionals and data scientists.
## Basic Architeture of MLFlow
![mlflow_arch](https://github.com/grish77/MLflow_basics/assets/83338844/7a1be006-5c35-467f-b229-3160af1e9de8)
The major objective of MLOps is to eliminate the communication gap between operations teams, who deploy and manage ML models in production environments, and data scientists, who create and train ML models. Organizations may guarantee that ML models are scalable, stable, and constantly developing by implementing MLOps principles.

1. _Simplified Deployment_:
The main focus of MLOps is to create reproducible environments by the use of tools like Docker or conda environments. By capturing and managing dependencies, including specific versions of libraries and frameworks, MLOps makes sure that the deployment environment of the model matches with the development environment. This can significantly reduce deployment issues that can occur due to variation in software versions and configurations.
MLflow enables us to package your trained models and their dependencies and metadata into a format that help in simplified model deployment. By using model format of MLflow, we can capture the architecture of the model, its necessary libraries, and pre-processing code making sure that the model is encapsulated with all its requirements. MLflow has built-in integration with popular deployment tools like Docker, Kubernetes, and Apache Spark, that enables seamless deployment of models to these platforms. It also supports serving models via REST APIs, that allows us to deploy models as web services.
2. _Model Versioning and Rollbacks_:
MLOps offer tools for managing model versions and makes it simple to roll back in case of problems. We can keep track of different model versions through model versioning, which makes it simpler to go back to an earlier deployment in case something goes wrong or performance degrades. This lowers the risks related to the deployment of new models into production. Model versioning involves assignment of a unique identifier or name to each version of the model and storing its metadata. The metadata includes information like the model's training data, hyperparameters, performance metrics, and associated artifacts. By maintaining model versioning, we can easily track the evolution of models, understand the impact of changes, and make informed decisions about using a specific version of the model or deploying specific versions into production.
Data scientists, developers, and other stakeholders may work together more effectively since it makes it possible to track, collaborate on, and reproduce different versions of ML artifacts. This surely increases transparency.
MLflow is an open-source platform that offers tools for managing the entire ML lifecycle, including model versioning and tracking. Although MLflow is primarily intended for experiment tracking and model maintenance, it can be used in concert with version control tools like Git to improve version control capabilities in MLOps processes. The version control repositories like Git includes or stores codes for training, preparing data, evaluating, and deploying ML models as well as configuration files that define dependencies, environment variables, and hyperparameters for the models. MLflow allows you to log and track ML experiments, including the code, data, hyperparameters, and metrics used during training. By using MLflow's tracking functionality, you can capture and record the details of each experiment run.
3. _Test Automation_
Another essential DevOps best practice is test automation, which frequently takes the form of integration testing and unit testing. These tests must be passed in order to deploy a new version. Automated, thorough testing can significantly increase the speed of production deployments, giving the team more confidence. In order to guarantee the quality and dependability of machine learning models used in production, test automation is essential. In order to validate models and related components across the development and deployment lifecycle, MLOps integrates test automation methods. The testing involve unit testing, integration testing, validation testing, and so on. The automation of these tests helps to identify and rectify issues early, promotes stability and reliability, and ensures that models meet the required quality standards before and after deployment.
4. _Continuous Integration and Continuous Deployment (CI/CD)_:
MLOps makes use of the CI/CD principles used in DevOps in order to automate the process of building, testing, and deploying machine learning models. Continuous Integration makes sure that code changes are regularly integrated and tested, reducing the risk of introducing errors during deployment. Continuous Deployment help in automation of the process of deploying models to production, making it faster and more reliable. CI/CD pipelines can be set up to trigger model deployment automatically whenever there are new changes or updates. Once the ML models are deployed, monitoring systems are set up to track their performance, detect anomalies, and collect feedback. This data is fed back into the CI/CD pipeline to trigger retraining, re-deployment, or other actions based on predefined conditions.

## MLOps vs DevOps
The terms "DevOps" and "MLOps" refer to related but different ideas in the fields of software development and machine learning, respectively. In order to accelerate software delivery, promote teamwork, and boost the stability and scalability of applications, DevOps focuses on optimizing the software development lifecycle by encouraging collaboration between development and operations teams. Continuous integration, continuous delivery/deployment, automated testing, infrastructure as code, and monitoring are all examples of DevOps principles.
However, MLOps focuses mainly on the management of the operations and lifecycle of machine learning models. It includes methods like version control of the machine learning artifacts, reproducibility for experiments, packaging of ML models, continuous integration and deployment of ML models, and model performance monitoring. MLOps is designed for managing the intricacies and unique requirements of software development projects, while DevOps covers a wider range of software development projects.

# MLflow_basics
MLflow is a versatile, expandable, open-source platform for managing workflows and artifacts across the machine learning lifecycle. It has built-in integrations with many popular ML libraries, but can be used with any library, algorithm, or deployment tool. It provides tools and APIs that help data scientists and developers track, reproduce, deploy, and collaborate on machine learning tasks.

### Some of the important features of MLflow involve:
1. _Model Tracking_:
   The MLflow Tracking component is an API and UI for logging parameters, code versions, metrics, and output files when running your machine learning code and for later visualizing the results. For many popular ML libraries,Model tracking can be easily implemented in the ML code by simply adding mlflow.autolog() function call. If we are using one of the supported libraries, this will automatically log the parameters, metrics, and artifacts of the run. 
MLflow Tracking is organized around the concept of runs, which are executions of some piece of data science code. In each run, the information about the model such as code version, start and end time, source, parameters, metrics, artifacts, etc. MLflow Tracking is organized around the concept of runs, which are executions of some piece of data science code.
![mlflow_tracking](https://github.com/grish77/MLflow_basics/assets/83338844/41fce815-f24d-4e04-9e12-c1b22922182f)
The diagram above depicts the tracking of multiple runs of ML experiments and comparision of the performance metrics of each run.

2. _MLFlow Models_:
MLflow Models is one of the features or components of MLflow that deals with model packaging and deployment in a very convenient way. It allows us to easily package and share ML models in a standardized format. With MLflow Models, we can define a model as a combination of code, environment dependencies, and serialized model artifacts. This definition is called a model "flavor." MLflow supports various flavors that correspond to different machine learning frameworks and tools. For example, it has built-in support for popular frameworks like TensorFlow, PyTorch, Scikit-learn, and XGBoost. Each flavor provides a set of conventions and tools for packaging and serving models trained with the respective framework. Once we define a model with a specific flavor, MLflow Models enable! us to deploy and serve the model in different deployment environments. It provides a simple way to deploy models as RESTful web services, allowing us to make predictions using HTTP requests. Additionally, MLflow Models supports integration with cloud platforms like Azure ML and AWS SageMaker, enabling us to deploy models to cloud-based serving infrastructure.
![model-details](https://github.com/grish77/MLflow_basics/assets/83338844/eddcaeb8-e327-4067-98ba-d219716664f7)
The artifacts folder of the model trained enabling MLflow contains the necessary information about the specific run of the model and its dependencies. As depicted in the above diagram, the model folder contains ML model file that is a JSON file that describes the model's metadata and provides information about the model's flavor, version, and dependencies. It typically includes details such as the model's name, version, input and output schema, and the flavor used for packaging the model. If we have used a Conda environment to train the model, MLflow may generate a Conda environment file that lists the dependencies required to run the model. This file specifies the necessary packages and their versions. The folder may also contain the information about the requiremens for running the model in other environments.

3. _MLFlow Projects_:
MLflow Projects provide a standardized format for packaging and reproducing code in a machine learning project. We can define dependencies, specify entry points, and run the code in a consistent environment across different platforms. Each project is simply a directory of files, or a Git repository, containing your code. MLflow can run some projects based on a convention for placing files in this directory (for example, a conda.yaml file is treated as a Conda environment), but we can describe our project in more detail by adding a MLproject file, which is a YAML formatted text file. Each project can specify several properties like name, entry points, environment, etc.



